{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSSL Lab 2 - Structural model, Kalman filtering and EM\n",
    "We will continue to work with the Global Mean Sea Level (GMSL) data that we got acquainted with in lab 1. The data is taken from https://climate.nasa.gov/vital-signs/sea-level/ and is available on LISAM in the file `sealevel.csv`.\n",
    "\n",
    "In this lab we will analyse this data using a structural time series model. We will first set up a model and implement a Kalman filter to infer the latet states of the model, as well doing long-term prediction. We will then implement a disturbance smoother and an expectation maximization algorithm to tune the parameters of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load a few packages that are useful for solving this lab assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas  # Loading data / handling data frames\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (12,8)  # Increase default size of plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Setting up a structural state space model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading and plotting data to remind ourselves what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pandas.read_csv('sealevel.csv',header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['GMSL'].values\n",
    "u = data['Year'].values\n",
    "ndata = len(y)\n",
    "plt.plot(u,y)\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('GMSL deviation (mm)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we will use a structural time series model to analys this data set. Specifically, we assume that the data $\\{y_t\\}_{t\\geq 1}$ is generated by\n",
    "\n",
    "\\begin{align}\n",
    "    y_t = \\mu_t + \\gamma_t + \\varepsilon_t\n",
    "\\end{align}\n",
    "\n",
    "where $\\mu_t$ is a trend component, $\\gamma_t$ is a seasonal component, and $\\varepsilon_t$ is an observation noise. The model is expressed using a state space representation,\n",
    "\n",
    "\\begin{align}\n",
    "    \\alpha_{t+1} &= T \\alpha_t + R\\eta_t, & \\eta_t&\\sim N(0,Q), \\\\\n",
    "    y_t &= Z \\alpha_t + \\varepsilon_t, & \\varepsilon_t&\\sim N(0,\\sigma_\\varepsilon^2).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q0:** Let $d = \\dim(\\alpha_t)$ denote the _state dimension_ and $d_\\eta = \\dim(\\eta_t)$ denote the dimension of the state noise. Then, what are the dimenisons of the matrices $T$, $R$, and $Z$ of the state space model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1:** Create the state space matrices $T_{[\\mu]}$, $R_{[\\mu]}$, and $Z_{[\\mu]}$ corresponding to the trend component $\\mu_t$. We assume a local linear trend (that is, of order $k=2$). \n",
    "\n",
    "_Hint:_ Use **2-dimensional** `numpy.ndarray`s of the correct sizes to represent all the matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_mu = \n",
    "R_mu = \n",
    "Z_mu = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2:** There is a yearly seasonal pattern present in the data. What should we set the periodicity $s$ of the seasonal component to, to capture this pattern?\n",
    "\n",
    "_Hint:_ Count the average number of observations per (whole) year and round to the closest integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3:** What is the _state dimension_ of a seasonal component with periodicity $s$? That is, how many states are needed in the corresponding state space representation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4:** Create the state space matrices $T_{[\\gamma]}$, $R_{[\\gamma]}$, and $Z_{[\\gamma]}$ corresponding to the seasonal component $\\gamma_t$. \n",
    "\n",
    "_Hint:_ Use **2-dimensional** `numpy.ndarray`s of the correct sizes to represent all the matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_gamma = \n",
    "R_gamma = \n",
    "Z_gamma = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5:** Using the matrices that you have constructed above, create the state space matrices for the complete structural time series model. Print out the shapes of the resulting system matrices and check that they correspond to what you expect (cf **Q0**).\n",
    "\n",
    "_Hint:_ Use `scipy.linalg.block_diag` and `numpy.concatenate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = \n",
    "R = \n",
    "Z = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to specify the variances of the process noise $\\eta_t$ and measurement noise $\\varepsilon_t$. Below, we will estimate (two of) these variances from data, but for now we set them arbitrarily to get an initial model to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some arbitrary noise values for now\n",
    "sigma_trend = 0.01\n",
    "sigma_seas = 1\n",
    "sigma_eps = 1\n",
    "Q = np.array([[sigma_trend**2, 0.], [0., sigma_seas**2]])  # Process noise covariance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to complete the model we need to specify the distribution of the initial state. This encodes our _a priori_ belief about the actual values of the trend and seasonality, i.e., before observing any data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6:** Set up the mean vector of the initial state $a_1 = \\mathbb{E}[\\alpha_1]$ such that:\n",
    "* The trend component starts at the first observation, $\\mathbb{E}[\\mu_1] = y_1$,\n",
    "* The slope of the trend is _a priori_ zero in expectation, $\\mathbb{E}[\\mu_1 - \\mu_0] = 0$,\n",
    "* The initial mean of all states related to the seasonal component are zero.\n",
    "\n",
    "Also, create an initial state covariance matrix $P_1 = \\text{Cov}(\\alpha_1)$ as an identity matrix of the correct dimension, multiplied with a large value (say, 100) to represent our uncertainty about the initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now defined all the matrices etc. that make up the structural state space model. For convenience, we can create an object of the class `LGSS` available in the module `tssltools_lab2` as a container for these quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tssltools_lab2 import LGSS  # Module available in LISAM\n",
    "model = LGSS(T, R, Q, Z, sigma_eps**2, a1, P1)\n",
    "\n",
    "help(model.get_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Kalman filtering for the structural model\n",
    "Now we have the data and a model available. Next, we will turn our attention to the inference problem, which is a central task when analysing time series data using the state space framework.\n",
    "\n",
    "State inference is the problem of estimating the unknown (latent) state variables given the data. For the time being we assume that the _model parameters_ are completely specified, according to above, and only consider how to estimate the states using the Kalman filter.\n",
    "\n",
    "In the questions below we will treat the first $n=800$ time steps as training data and the remaining $m=197$ observations as validation data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 800\n",
    "m = ndata-n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Q7:** Complete the Kalman filter implementation below. The function should be able to handle missing observations, which are encoded as \"not a number\", i.e. `y[t] = np.nan` for certain time steps `t`. \n",
    "\n",
    "_Hint:_ The Kalman filter involves a lot of matrix-matrix and matrix-vector multiplications. It turns out to be convient to store sequences of vectors (such as the predicted and filtered state estimates) as `(d,1,n)` arrays, instead of `(d,n)` or `(n,d)` arrays. In this way the matrix multiplications will result in 2d-arrays of the correct shapes without having to use a lot of explicit `reshape`. However, clearly, this is just a matter of coding style preferences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tssltools_lab2 import kfs_res  # Module available in LISAM. kfs_res is a container class for storing the result.\n",
    "\n",
    "def kalman_filter(y, model: LGSS):\n",
    "    \"\"\"Kalman filter for LGSS model with one-dimensional observation.\n",
    "\n",
    "    :param y: (n,) array of observations. May contain nan, which encodes missing observations.\n",
    "    :param model: LGSS object with the model specification.\n",
    "    \n",
    "    :return kfs_res: Container class with member variables,\n",
    "        alpha_pred: (d,1,n) array of predicted state means.\n",
    "        P_pred: (d,d,n) array of predicted state covariances.\n",
    "        alpha_filt: (d,1,n) array of filtered state means.\n",
    "        P_filt: (d,d,n) array of filtered state covariances.\n",
    "        y_pred: (n,) array of means of p(y_t | y_{1:t-1})\n",
    "        F_pred: (n,) array of variances of p(y_t | y_{1:t-1})\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(y)\n",
    "    d = model.d  # State dimension\n",
    "    alpha_pred = np.zeros((d, 1, n))\n",
    "    P_pred = np.zeros((d, d, n))\n",
    "    alpha_filt = np.zeros((d, 1, n))\n",
    "    P_filt = np.zeros((d, d, n))\n",
    "    y_pred = np.zeros(n)\n",
    "    F_pred = np.zeros(n)\n",
    "\n",
    "    T, R, Q, Z, H, a1, P1 = model.get_params()  # Get all model parameters (for brevity)\n",
    "\n",
    "    for t in range(n):\n",
    "        # Time update (predict)\n",
    "        if t == 0:  # Initialize predictions at first time step\n",
    "            # ADD CODE HERE\n",
    "        else:  # All consecutive time steps\n",
    "            # ADD CODE HERE\n",
    "\n",
    "        # Compute prediction of current output\n",
    "        y_pred[t] = ... # complete this line\n",
    "        F_pred[t] = ... # complete this line\n",
    "\n",
    "        # Measurement update (correct)\n",
    "        if np.isnan(y[t]):  # Handle missing data\n",
    "            # ADD CODE HERE\n",
    "        else:\n",
    "            # ADD CODE HERE\n",
    "\n",
    "    # Container for storing all results\n",
    "    kf = kfs_res(alpha_pred, P_pred, alpha_filt, P_filt, y_pred, F_pred)\n",
    "    return kf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8:** Use the Kalman filter to infer the states of the structural time series applied to the sealevel data. Run the filter on the training data (i.e., first $n=700$ time steps), followed by a long-range prediction of $y_t$ for the remaining time points. \n",
    "\n",
    "Generate a plot which shows:\n",
    "1. The data $y_{1:n+m}$,\n",
    "2. The one-step predictions $\\hat y_{t|t-1} \\pm 1 $ standard deviation for the training data, i.e., $t = 1,...,n$,\n",
    "3. The long-range predictions $\\hat y_{t|n} \\pm 1 $ standard deviation for the validation data, i.e., $t= n+1,...,n+m$,\n",
    "4. A vertical line indicating the switch between training and validation data, using, e.g., `plt.axvline(x=u[n])`.\n",
    "\n",
    "_Hint:_ It is enough to call the `kalman_filter` function once. Make use of the missing data functionality!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q9:** Based on the output of the Kalman filter, compute the training data log-likelihood $\\log p(y_{1:n})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Identifying the noise variances using the EM algorithm\n",
    "So far we have used fixed model parameters when running the filter. In this section we will see how the model parameters can be learnt from data using the EM algorithm. Specifically, we will try to learn the variance of the state noise affecting the seasonal component as well as the variance of the observation noise,\n",
    "\n",
    "\\begin{align}\n",
    "    \\theta = (\\sigma_\\gamma^2, \\sigma_\\varepsilon^2).\n",
    "\\end{align}\n",
    "\n",
    "For brevity, the variance of the trend component $\\sigma_\\mu^2$ is fixed to the value $\\sigma_\\mu^2 = 0.01^2$ as above. (See Appendix A below for an explanation.)\n",
    "\n",
    "Recall that we consider $y_{1:n}$ as the training data, i.e., we will estimate $\\theta$ using only the first $n=800$ observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q10:** Which optimization problem is it that the EM algorithm is designed to solve? Complete the line below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:** $\\hat\\theta = \\arg\\max_{\\theta} ????? $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q11:** Write down the updating equations on closed form for the M-step in the EM algorithm.\n",
    "\n",
    "_Hint: Look at Exercise Session 2_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the EM algorithm we need to solve a _smoothing problem_. The Kalman filter that we implemented above is based only on a forward propagation of information. The _smoother_ complements the forward filter with a backward pass to compute refined state estimates. Specifically, the smoothed state estimates comprise the mean and covariances of\n",
    "\n",
    "\\begin{align}\n",
    "    &p(\\alpha_t \\mid y_{1:n}), & t=&1,\\dots,n\n",
    "\\end{align}\n",
    "\n",
    "Furthermore, when implementing the EM algorithm it is convenient to work with the (closely related) smoothed estimates of the disturbances, i.e., the state and measurement noise,\n",
    "\n",
    "\\begin{align}\n",
    "    &p(\\eta_t \\mid y_{1:n}), & t=&1,\\dots,n-1 \\\\\n",
    "    &p(\\varepsilon_t \\mid y_{1:n}), & t=&1,\\dots,n\n",
    "\\end{align}\n",
    "\n",
    "An implementation of a state and disturbance smoother is available in the `tssltools_lab2` module. You may use this when implementing the EM algorithm below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tssltools_lab2 import kalman_smoother\n",
    "help(kalman_smoother)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q12:** Implement an EM algorithm by completing the code below. Run the algorithm for 100 iterations and plot the traces of the parameter estimates, i.e., the values $\\theta_r$, for $r = 0,\\dots,100$.\n",
    "\n",
    "_Note:_ When running the Kalman filter as part of the EM loop you should only filter the _training data_ (i.e. excluding the prediction for validation data).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter = 100\n",
    "\n",
    "def compute_theta(ks):\n",
    "    \"\"\"Implements the M-step of the EM-algorithm, based on the Kalman smoother results (ks). See Q11.\"\"\"\n",
    "    theta_new = ... # complete this line\n",
    "    return theta_new\n",
    "\n",
    "\n",
    "for r in range(1,num_iter):\n",
    "    # E-step\n",
    "    # ADD CODE HERE\n",
    "    \n",
    "    # M-step\n",
    "    # ADD CODE HERE\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Further analysing the data\n",
    "We will now fix the model according to the final output from the EM algorithm and further analyse the data using this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q13:** Rerun the Kalman filter to compute a _long range prediction for the validation data points,_ analogously to **Q8** (you can copy-paste code from that question). That is, generate a plot which shows:\n",
    "1. The data $y_{1:n+m}$,\n",
    "2. The one-step predictions $\\hat y_{t|t-1} \\pm 1 $ standard deviation for the training data, i.e., $t = 1,...,n$,\n",
    "3. The long-range predictions $\\hat y_{t|n} \\pm 1 $ standard deviation for the validation data, i.e., $t= n+1,...,n+m$,\n",
    "4. A vertical line indicating the switch between training and validation data, using, e.g., `plt.axvline(x=u[n])`.\n",
    "\n",
    "Furthermore, compute the training data log-likelihood $\\log p(y_{1:n})$ using the estimated model (cf. **Q9**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can view the model for the data $y_t$ as being comprised of an underlying \"signal\", $s_t = \\mu_t + \\gamma_t$ plus observation noise $\\varepsilon_t$\n",
    "\n",
    "\\begin{align}\n",
    "    y_t = s_t + \\varepsilon_t\n",
    "\\end{align}\n",
    "\n",
    "We can obtain refined, _smoothed,_ estimates of this signal by conditioning on all the training data $y_{1:n}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q14:** Run a Kalman smoother to compute smoothed estimates of the signal, $\\mathbb{E}[s_t | y_{1:n}]$, conditionally on all the _training data_. Then, similarly to above, plot the following:\n",
    "1. The data $y_{1:n+m}$,\n",
    "2. The smoothed estimates $\\mathbb{E}[s_t | y_{1:n}] \\pm 1 $ standard deviation for the training data, i.e., $t = 1,...,n$,\n",
    "3. The predictions $\\mathbb{E}[s_t | y_{1:n}] \\pm 1 $ standard deviation for the validation data, i.e., $t = n+1,...,n+m$,\n",
    "4. A vertical line indicating the switch between training and validation data, using, e.g., `plt.axvline(x=u[n])`.\n",
    "\n",
    "_Hint:_ Express $s_t$ in terms of $\\alpha_t$. Based on this expression, compute the smoothed mean and variance of $s_t$ based on the smoothed mean and covariance of $\\alpha_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q15:** Explain, using a few sentences, the qualitative differences (or similarities) between the Kalman filter predictions plotted in **Q13** and the smoothed signal estimates plotted in **Q14** for,\n",
    "1. Training data points, $t \\leq n$\n",
    "2. Validation data points, $t > n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can shed additional light on the properties of the process under study by further decomposing the signal into its trend and seasonal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q16:** Using the results of the state smoother, compute and plot the _smoothed estimates_ of the two signal components, i.e.:\n",
    "\n",
    "1. Trend: $\\hat \\mu_{t|n} = \\mathbb{E}[\\mu_t | y_{1:n}]$ for $t = 1,\\dots,n$\n",
    "2. Seasonal: $\\hat \\gamma_{t|n} = \\mathbb{E}[\\gamma_t | y_{1:n}]$ for $t = 1,\\dots,n$\n",
    "\n",
    "_(You don't have to include confidence intervals here if don't want to, for brevity.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Missing data\n",
    "We conclude this section by illustrating one of the key merits of the state space approach to time series analysis, namely the simplicity of handling missing data. To this end we will assume that a chunk of observations in the middle of the training data is missing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q17:** Let the values $y_{t}$ for $ 300 < t \\leq 400$ be missing (set to `np.nan`). Modify the data and rerun the Kalman filter and smoother. Plot,\n",
    "1. The Kalman filter predictions, analogously to **Q8**\n",
    "2. The Kalman smoother predictions, analogously to **Q13**\n",
    "\n",
    "Comment on the qualitative differences between the filter and smoother estimates and explain what you see (in a couple of sentences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix A. Why didn't we learn the trend noise variance as well?\n",
    "In the assignment above we have fixed $\\sigma_\\mu$ to a small value. Conceptually it would have been straightforward to learn also this parameter with the EM algorithm. However, unfortunately, the maximum likelihood estimate of $\\sigma_\\mu$ often ends up being too large to result in accurate _long term predictions_. The reason for this issue is that the structural model\n",
    "\n",
    "\\begin{align}\n",
    "    y_t = \\mu_t + \\gamma_t + \\varepsilon_t\n",
    "\\end{align}\n",
    "\n",
    "is not a perfect description of reality. As a consequence, when learning the parameters the mismatch between the model and the data is compensated for by increasing the noise variances. This results in a trend component which does not only capture the long term trends of the data, but also seemingly random variations due to a model misspecification, possibly resulting in poor _long range predictions_.\n",
    "\n",
    "Kitagawa (Introduction to Time Series Modeling, CRC Press, 2010, Section 12.3) discusses this issue and proposes two solutions. The first is a simple and pragmatic one: simply fix $\\sigma_\\mu^2$ to a value smaller than the maximum likelihood estimate. This is the approach we have taken in this assignment. The issue is of course that in practice it is hard to know what value to pick, which boild down to manual trial and error (or, if you are lucky, the designer of the lab assignment will tell you which value to use!).\n",
    "\n",
    "The second, more principled, solution proposed by Kitagawa is to augment the model with a stationary AR component as well. That is, we model\n",
    "\n",
    "\\begin{align}\n",
    "    y_t = \\mu_t + \\gamma_t + \\nu_t + \\varepsilon_t\n",
    "\\end{align}\n",
    "\n",
    "where $\\nu_t \\sim$ AR$(p)$. By doing so, the stationary AR component can compensate for the discrepancies between the original structural model and the \"true data generating process\". It is straightforward to include this new component in the state space representation (how?) and to run the Kalman filter and smoother on the resulting model. Indeed, this is one of the beauties with working with the state space representation of time series data! However, the M-step of the EM algorithm becomes a bit more involved if we want to use the method to estimate also the AR coefficients of the $\\nu$-component, which is beyond the scope of this lab assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
